{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project to Upload Files to GCS using Pandas\n",
    "\n",
    "As part of the series of lectures we will see how to upload files to GCS using Python Pandas. We will be using `glob`, `os`, and `pandas` to build the application logic.\n",
    "\n",
    "Here are the design details.\n",
    "* First, we need to get list of file names from the local file system to upload.\n",
    "* As we want to have right column names for our data set, we need to ensure that the column names are extracted from **schemas.json** file in `data/retail_db`.\n",
    "* Once we get the file names, we can use `pd.read_csv` with `names` to create Dataframe and then write to target GCS location using `parquet` file format.\n",
    "* We will use metadata or data driven development approach to take care of uploading all the files related to retail to GCS.\n",
    "* Blobs or Files using Parquet format will be named using file names as reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Caught CTRL-C (signal 2) - exiting\n"
     ]
    }
   ],
   "source": [
    "!gsutil rm -r gs://airetail_mld/retail_db_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Caught CTRL-C (signal 2) - exiting\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://airetail_mld/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return list of files ending with 'part-00000'\n",
    "def get_file_names(src_base_dir):\n",
    "    items = glob.glob(f'{src_base_dir}/**', recursive=True)\n",
    "    return list(filter(lambda item: os.path.isfile(item) and item.endswith('part-00000'), items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/retail_db/customers/part-00000',\n",
       " '../../data/retail_db/products/part-00000',\n",
       " '../../data/retail_db/departments/part-00000',\n",
       " '../../data/retail_db/order_items/part-00000',\n",
       " '../../data/retail_db/orders/part-00000',\n",
       " '../../data/retail_db/categories/part-00000']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_base_dir = '../../data/retail_db'\n",
    "\n",
    "get_file_names(src_base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'departments': [{'column_name': 'department_id',\n",
       "   'data_type': 'integer',\n",
       "   'column_position': 1},\n",
       "  {'column_name': 'department_name',\n",
       "   'data_type': 'string',\n",
       "   'column_position': 2}],\n",
       " 'categories': [{'column_name': 'category_id',\n",
       "   'data_type': 'integer',\n",
       "   'column_position': 1},\n",
       "  {'column_name': 'category_department_id',\n",
       "   'data_type': 'integer',\n",
       "   'column_position': 2},\n",
       "  {'column_name': 'category_name',\n",
       "   'data_type': 'string',\n",
       "   'column_position': 3}],\n",
       " 'orders': [{'column_name': 'order_id',\n",
       "   'data_type': 'integer',\n",
       "   'column_position': 1},\n",
       "  {'column_name': 'order_date', 'data_type': 'string', 'column_position': 2},\n",
       "  {'column_name': 'order_customer_id',\n",
       "   'data_type': 'timestamp',\n",
       "   'column_position': 3},\n",
       "  {'column_name': 'order_status',\n",
       "   'data_type': 'string',\n",
       "   'column_position': 4}],\n",
       " 'products': [{'column_name': 'product_id',\n",
       "   'data_type': 'integer',\n",
       "   'column_position': 1},\n",
       "  {'column_name': 'product_cateogry_id',\n",
       "   'data_type': 'integer',\n",
       "   'column_position': 2},\n",
       "  {'column_name': 'product_name', 'data_type': '', 'column_position': 3},\n",
       "  {'column_name': 'product_description',\n",
       "   'data_type': 'string',\n",
       "   'column_position': 4},\n",
       "  {'column_name': 'product_price', 'data_type': 'float', 'column_position': 5},\n",
       "  {'column_name': 'product_image',\n",
       "   'data_type': 'string',\n",
       "   'column_position': 6}],\n",
       " 'customers': [{'column_name': 'customer_id',\n",
       "   'data_type': 'integer',\n",
       "   'column_position': 1},\n",
       "  {'column_name': 'customer_fname',\n",
       "   'data_type': 'string',\n",
       "   'column_position': 2},\n",
       "  {'column_name': 'customer_lname',\n",
       "   'data_type': 'string',\n",
       "   'column_position': 3},\n",
       "  {'column_name': 'customer_email',\n",
       "   'data_type': 'string',\n",
       "   'column_position': 4},\n",
       "  {'column_name': 'customer_password',\n",
       "   'data_type': 'string',\n",
       "   'column_position': 5},\n",
       "  {'column_name': 'customer_street',\n",
       "   'data_type': 'string',\n",
       "   'column_position': 6},\n",
       "  {'column_name': 'customer_city',\n",
       "   'data_type': 'string',\n",
       "   'column_position': 7},\n",
       "  {'column_name': 'customer_state',\n",
       "   'data_type': 'string',\n",
       "   'column_position': 8},\n",
       "  {'column_name': 'customer_zipcode',\n",
       "   'data_type': 'integer',\n",
       "   'column_position': 9}],\n",
       " 'order_items': [{'column_name': 'order_item_id',\n",
       "   'data_type': 'integer',\n",
       "   'column_position': 1},\n",
       "  {'column_name': 'order_item_order_id',\n",
       "   'data_type': 'integer',\n",
       "   'column_position': 2},\n",
       "  {'column_name': 'order_item_product_id',\n",
       "   'data_type': 'integer',\n",
       "   'column_position': 3},\n",
       "  {'column_name': 'order_item_quantity',\n",
       "   'data_type': 'integer',\n",
       "   'column_position': 4},\n",
       "  {'column_name': 'order_item_subtotal',\n",
       "   'data_type': 'float',\n",
       "   'column_position': 5},\n",
       "  {'column_name': 'order_item_product_price',\n",
       "   'data_type': 'float',\n",
       "   'column_position': 6}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemas = json.load(open('../../data/retail_db/schemas.json'))\n",
    "schemas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a look at the orders table, we can see that it is a json array: list of jsons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'column_name': 'order_id', 'data_type': 'integer', 'column_position': 1},\n",
       " {'column_name': 'order_date', 'data_type': 'string', 'column_position': 2},\n",
       " {'column_name': 'order_customer_id',\n",
       "  'data_type': 'timestamp',\n",
       "  'column_position': 3},\n",
       " {'column_name': 'order_status', 'data_type': 'string', 'column_position': 4}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look at the orders table\n",
    "schemas['orders']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'column_name': 'order_id', 'data_type': 'integer', 'column_position': 1},\n",
       " {'column_name': 'order_date', 'data_type': 'string', 'column_position': 2},\n",
       " {'column_name': 'order_customer_id',\n",
       "  'data_type': 'timestamp',\n",
       "  'column_position': 3},\n",
       " {'column_name': 'order_status', 'data_type': 'string', 'column_position': 4}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# key: sorted by the colum_position value\n",
    "ds_schema = sorted(schemas['orders'], key=lambda col: col['column_position'])\n",
    "ds_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['order_id', 'order_date', 'order_customer_id', 'order_status']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list comprehension of ordered column names\n",
    "[col['column_name'] for col in ds_schema]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all in get_column_names function: use json and data base as inputs and returns ordered column names \n",
    "def get_column_names(schemas_file, ds_name):\n",
    "    schemas = json.load(open(schemas_file))\n",
    "    ds_schema = sorted(schemas[ds_name], key=lambda col: col['column_position'])\n",
    "    columns = [col['column_name'] for col in ds_schema]\n",
    "    return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['order_id', 'order_date', 'order_customer_id', 'order_status']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_column_names('../../data/retail_db/schemas.json', 'orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns for departments are: department_id, department_name\n",
      "columns for categories are: category_id, category_department_id, category_name\n",
      "columns for products are: product_id, product_cateogry_id, product_name, product_description, product_price, product_image\n",
      "columns for customers are: customer_id, customer_fname, customer_lname, customer_email, customer_password, customer_street, customer_city, customer_state, customer_zipcode\n",
      "columns for orders are: order_id, order_date, order_customer_id, order_status\n",
      "columns for order_items are: order_item_id, order_item_order_id, order_item_product_id, order_item_quantity, order_item_subtotal, order_item_product_price\n"
     ]
    }
   ],
   "source": [
    "for ds in [\n",
    "    'departments', 'categories', 'products',\n",
    "    'customers', 'orders', 'order_items'\n",
    "]:\n",
    "    column_names = get_column_names('../../data/retail_db/schemas.json', ds)\n",
    "    print(f'''columns for {ds} are: {', '.join(column_names)}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/retail_db/customers/part-00000',\n",
       " '../../data/retail_db/products/part-00000',\n",
       " '../../data/retail_db/departments/part-00000',\n",
       " '../../data/retail_db/order_items/part-00000',\n",
       " '../../data/retail_db/orders/part-00000',\n",
       " '../../data/retail_db/categories/part-00000']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_base_dir = '../../data/retail_db'\n",
    "schemas_file = '../../data/retail_db/schemas.json'\n",
    "bucket = 'airetail_mld'\n",
    "files = get_file_names(src_base_dir)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/retail_db/customers/part-00000'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = files[0]\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'customers/part-00000'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keeps last 2 parts of file path: data set and file name\n",
    "'/'.join(file.split('/')[-2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'customers'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data set\n",
    "ds_name = file.split('/')[-2]\n",
    "ds_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customer_id',\n",
       " 'customer_fname',\n",
       " 'customer_lname',\n",
       " 'customer_email',\n",
       " 'customer_password',\n",
       " 'customer_street',\n",
       " 'customer_city',\n",
       " 'customer_state',\n",
       " 'customer_zipcode']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check column names\n",
    "columns = get_column_names(schemas_file, ds_name)\n",
    "columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following loop does the following:\n",
    "1. Looks for all files directory\n",
    "2. Keeps dataset name and file name\n",
    "3. Stores dataset name\n",
    "4. Define GCS URI\n",
    "5. Extract dataset's column names\n",
    "6. Read file and save into GCS as parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file ../../data/retail_db/customers/part-00000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m columns \u001b[39m=\u001b[39m get_column_names(schemas_file, ds_name)\n\u001b[1;32m     12\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(file, names\u001b[39m=\u001b[39mcolumns)\n\u001b[0;32m---> 13\u001b[0m df\u001b[39m.\u001b[39;49mto_parquet(blob_name, index\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/Desktop/data-engineering-on-gcp/.conda/lib/python3.9/site-packages/pandas/core/frame.py:2888\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2801\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2802\u001b[0m \u001b[39mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2884\u001b[0m \u001b[39m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   2885\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2886\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparquet\u001b[39;00m \u001b[39mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 2888\u001b[0m \u001b[39mreturn\u001b[39;00m to_parquet(\n\u001b[1;32m   2889\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2890\u001b[0m     path,\n\u001b[1;32m   2891\u001b[0m     engine,\n\u001b[1;32m   2892\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   2893\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   2894\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m   2895\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2896\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2897\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/data-engineering-on-gcp/.conda/lib/python3.9/site-packages/pandas/io/parquet.py:411\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[1;32m    409\u001b[0m path_or_buf: FilePath \u001b[39m|\u001b[39m WriteBuffer[\u001b[39mbytes\u001b[39m] \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m path\n\u001b[0;32m--> 411\u001b[0m impl\u001b[39m.\u001b[39;49mwrite(\n\u001b[1;32m    412\u001b[0m     df,\n\u001b[1;32m    413\u001b[0m     path_or_buf,\n\u001b[1;32m    414\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    415\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    416\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m    417\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    418\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    419\u001b[0m )\n\u001b[1;32m    421\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, io\u001b[39m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m~/Desktop/data-engineering-on-gcp/.conda/lib/python3.9/site-packages/pandas/io/parquet.py:189\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi\u001b[39m.\u001b[39mparquet\u001b[39m.\u001b[39mwrite_to_dataset(\n\u001b[1;32m    181\u001b[0m             table,\n\u001b[1;32m    182\u001b[0m             path_or_handle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    186\u001b[0m         )\n\u001b[1;32m    187\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m         \u001b[39m# write to single output file\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi\u001b[39m.\u001b[39;49mparquet\u001b[39m.\u001b[39;49mwrite_table(\n\u001b[1;32m    190\u001b[0m             table, path_or_handle, compression\u001b[39m=\u001b[39;49mcompression, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    191\u001b[0m         )\n\u001b[1;32m    192\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[39mif\u001b[39;00m handles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/data-engineering-on-gcp/.conda/lib/python3.9/site-packages/pyarrow/parquet/core.py:3093\u001b[0m, in \u001b[0;36mwrite_table\u001b[0;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, column_encoding, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, **kwargs)\u001b[0m\n\u001b[1;32m   3070\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   3071\u001b[0m     \u001b[39mwith\u001b[39;00m ParquetWriter(\n\u001b[1;32m   3072\u001b[0m             where, table\u001b[39m.\u001b[39mschema,\n\u001b[1;32m   3073\u001b[0m             filesystem\u001b[39m=\u001b[39mfilesystem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3091\u001b[0m             store_schema\u001b[39m=\u001b[39mstore_schema,\n\u001b[1;32m   3092\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mas\u001b[39;00m writer:\n\u001b[0;32m-> 3093\u001b[0m         writer\u001b[39m.\u001b[39mwrite_table(table, row_group_size\u001b[39m=\u001b[39mrow_group_size)\n\u001b[1;32m   3094\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   3095\u001b[0m     \u001b[39mif\u001b[39;00m _is_path_like(where):\n",
      "File \u001b[0;32m~/Desktop/data-engineering-on-gcp/.conda/lib/python3.9/site-packages/pyarrow/parquet/core.py:1018\u001b[0m, in \u001b[0;36mParquetWriter.__exit__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1018\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclose()\n\u001b[1;32m   1019\u001b[0m     \u001b[39m# return false since we want to propagate exceptions\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/data-engineering-on-gcp/.conda/lib/python3.9/site-packages/pyarrow/parquet/core.py:1091\u001b[0m, in \u001b[0;36mParquetWriter.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata_collector\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter\u001b[39m.\u001b[39mmetadata)\n\u001b[1;32m   1090\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_handle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1091\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_handle\u001b[39m.\u001b[39;49mclose()\n",
      "File \u001b[0;32m~/Desktop/data-engineering-on-gcp/.conda/lib/python3.9/site-packages/pyarrow/io.pxi:189\u001b[0m, in \u001b[0;36mpyarrow.lib.NativeFile.close\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/data-engineering-on-gcp/.conda/lib/python3.9/site-packages/fsspec/spec.py:1789\u001b[0m, in \u001b[0;36mAbstractBufferedFile.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1787\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1788\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforced:\n\u001b[0;32m-> 1789\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflush(force\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1791\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1792\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfs\u001b[39m.\u001b[39minvalidate_cache(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath)\n",
      "File \u001b[0;32m~/Desktop/data-engineering-on-gcp/.conda/lib/python3.9/site-packages/fsspec/spec.py:1655\u001b[0m, in \u001b[0;36mAbstractBufferedFile.flush\u001b[0;34m(self, force)\u001b[0m\n\u001b[1;32m   1653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffset \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m   1654\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1655\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initiate_upload()\n\u001b[1;32m   1656\u001b[0m \u001b[39mexcept\u001b[39;00m:  \u001b[39m# noqa: E722\u001b[39;00m\n\u001b[1;32m   1657\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclosed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/data-engineering-on-gcp/.conda/lib/python3.9/site-packages/gcsfs/core.py:1592\u001b[0m, in \u001b[0;36mGCSFile._initiate_upload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1590\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_initiate_upload\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1591\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Create multi-upload\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1592\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocation \u001b[39m=\u001b[39m sync(\n\u001b[1;32m   1593\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgcsfs\u001b[39m.\u001b[39;49mloop,\n\u001b[1;32m   1594\u001b[0m         initiate_upload,\n\u001b[1;32m   1595\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgcsfs,\n\u001b[1;32m   1596\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbucket,\n\u001b[1;32m   1597\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey,\n\u001b[1;32m   1598\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontent_type,\n\u001b[1;32m   1599\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata,\n\u001b[1;32m   1600\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_key_metadata,\n\u001b[1;32m   1601\u001b[0m         timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout,\n\u001b[1;32m   1602\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/data-engineering-on-gcp/.conda/lib/python3.9/site-packages/fsspec/asyn.py:88\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m asyncio\u001b[39m.\u001b[39mrun_coroutine_threadsafe(_runner(event, coro, result, timeout), loop)\n\u001b[1;32m     86\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[39m# this loops allows thread to get interrupted\u001b[39;00m\n\u001b[0;32m---> 88\u001b[0m     \u001b[39mif\u001b[39;00m event\u001b[39m.\u001b[39;49mwait(\u001b[39m1\u001b[39;49m):\n\u001b[1;32m     89\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/data-engineering-on-gcp/.conda/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    582\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/Desktop/data-engineering-on-gcp/.conda/lib/python3.9/threading.py:316\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    315\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 316\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    317\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "src_base_dir = '../../data/retail_db'\n",
    "tgt_base_dir = 'retail_db_parquet'\n",
    "schemas_file = '../../data/retail_db/schemas.json'\n",
    "bucket = 'airetail_mld'\n",
    "files = get_file_names(src_base_dir)\n",
    "\n",
    "for file in files:\n",
    "    print(f'Uploading file {file}')\n",
    "    blob_suffix = '/'.join(file.split('/')[-2:])\n",
    "    ds_name = file.split('/')[-2]\n",
    "    blob_name = f'gs://{bucket}/{tgt_base_dir}/{blob_suffix}.snappy.parquet'\n",
    "    columns = get_column_names(schemas_file, ds_name)\n",
    "    \n",
    "    df = pd.read_csv(file, names=columns)\n",
    "    df.to_parquet(blob_name, index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Very important!**\n",
    "\n",
    "Always double check the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1264.37s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Caught CTRL-C (signal 2) - exiting\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls -r gs://airetail/retail_db_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>11599</td>\n",
       "      <td>CLOSED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>256</td>\n",
       "      <td>PENDING_PAYMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>12111</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>8827</td>\n",
       "      <td>CLOSED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>11318</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68878</th>\n",
       "      <td>68879</td>\n",
       "      <td>2014-07-09 00:00:00.0</td>\n",
       "      <td>778</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68879</th>\n",
       "      <td>68880</td>\n",
       "      <td>2014-07-13 00:00:00.0</td>\n",
       "      <td>1117</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68880</th>\n",
       "      <td>68881</td>\n",
       "      <td>2014-07-19 00:00:00.0</td>\n",
       "      <td>2518</td>\n",
       "      <td>PENDING_PAYMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68881</th>\n",
       "      <td>68882</td>\n",
       "      <td>2014-07-22 00:00:00.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>ON_HOLD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68882</th>\n",
       "      <td>68883</td>\n",
       "      <td>2014-07-23 00:00:00.0</td>\n",
       "      <td>5533</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68883 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0                      1      2                3\n",
       "0          1  2013-07-25 00:00:00.0  11599           CLOSED\n",
       "1          2  2013-07-25 00:00:00.0    256  PENDING_PAYMENT\n",
       "2          3  2013-07-25 00:00:00.0  12111         COMPLETE\n",
       "3          4  2013-07-25 00:00:00.0   8827           CLOSED\n",
       "4          5  2013-07-25 00:00:00.0  11318         COMPLETE\n",
       "...      ...                    ...    ...              ...\n",
       "68878  68879  2014-07-09 00:00:00.0    778         COMPLETE\n",
       "68879  68880  2014-07-13 00:00:00.0   1117         COMPLETE\n",
       "68880  68881  2014-07-19 00:00:00.0   2518  PENDING_PAYMENT\n",
       "68881  68882  2014-07-22 00:00:00.0  10000          ON_HOLD\n",
       "68882  68883  2014-07-23 00:00:00.0   5533         COMPLETE\n",
       "\n",
       "[68883 rows x 4 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../../data/retail_db/orders/part-00000', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_date</th>\n",
       "      <th>order_customer_id</th>\n",
       "      <th>order_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>11599</td>\n",
       "      <td>CLOSED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>256</td>\n",
       "      <td>PENDING_PAYMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>12111</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>8827</td>\n",
       "      <td>CLOSED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2013-07-25 00:00:00.0</td>\n",
       "      <td>11318</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68878</th>\n",
       "      <td>68879</td>\n",
       "      <td>2014-07-09 00:00:00.0</td>\n",
       "      <td>778</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68879</th>\n",
       "      <td>68880</td>\n",
       "      <td>2014-07-13 00:00:00.0</td>\n",
       "      <td>1117</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68880</th>\n",
       "      <td>68881</td>\n",
       "      <td>2014-07-19 00:00:00.0</td>\n",
       "      <td>2518</td>\n",
       "      <td>PENDING_PAYMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68881</th>\n",
       "      <td>68882</td>\n",
       "      <td>2014-07-22 00:00:00.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>ON_HOLD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68882</th>\n",
       "      <td>68883</td>\n",
       "      <td>2014-07-23 00:00:00.0</td>\n",
       "      <td>5533</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68883 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       order_id             order_date  order_customer_id     order_status\n",
       "0             1  2013-07-25 00:00:00.0              11599           CLOSED\n",
       "1             2  2013-07-25 00:00:00.0                256  PENDING_PAYMENT\n",
       "2             3  2013-07-25 00:00:00.0              12111         COMPLETE\n",
       "3             4  2013-07-25 00:00:00.0               8827           CLOSED\n",
       "4             5  2013-07-25 00:00:00.0              11318         COMPLETE\n",
       "...         ...                    ...                ...              ...\n",
       "68878     68879  2014-07-09 00:00:00.0                778         COMPLETE\n",
       "68879     68880  2014-07-13 00:00:00.0               1117         COMPLETE\n",
       "68880     68881  2014-07-19 00:00:00.0               2518  PENDING_PAYMENT\n",
       "68881     68882  2014-07-22 00:00:00.0              10000          ON_HOLD\n",
       "68882     68883  2014-07-23 00:00:00.0               5533         COMPLETE\n",
       "\n",
       "[68883 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet('gs://airetail_mld/retail_db_parquet/orders/part-00000.snappy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of departments in local files system is (6, 2)\n",
      "Shape of categories in local files system is (58, 3)\n",
      "Shape of products in local files system is (1345, 6)\n",
      "Shape of customers in local files system is (12435, 9)\n",
      "Shape of orders in local files system is (68883, 4)\n",
      "Shape of order_items in local files system is (172198, 6)\n"
     ]
    }
   ],
   "source": [
    "for ds in [\n",
    "    'departments', 'categories', 'products',\n",
    "    'customers', 'orders', 'order_items'\n",
    "]:\n",
    "    df = pd.read_csv(f'../../data/retail_db/{ds}/part-00000', header=None)\n",
    "    print(f'''Shape of {ds} in local files system is {df.shape}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of departments in gcs is (6, 2)\n",
      "Shape of categories in gcs is (58, 3)\n",
      "Shape of products in gcs is (1345, 6)\n",
      "Shape of customers in gcs is (12435, 9)\n",
      "Shape of orders in gcs is (68883, 4)\n",
      "Shape of order_items in gcs is (172198, 6)\n"
     ]
    }
   ],
   "source": [
    "for ds in [\n",
    "    'departments', 'categories', 'products',\n",
    "    'customers', 'orders', 'order_items'\n",
    "]:\n",
    "    df = pd.read_parquet(f'gs://{bucket}/{tgt_base_dir}/{ds}/part-00000.snappy.parquet')\n",
    "    print(f'''Shape of {ds} in gcs is {df.shape}''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('deg-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a9d607f6995d470a72ac62c14cbba774ae3a8ede2bb7bb3a284130b245adccf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
