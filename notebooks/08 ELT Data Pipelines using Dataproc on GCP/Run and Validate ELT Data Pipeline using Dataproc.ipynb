{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run and Validate ELT Data Pipeline using Dataproc\n",
    "\n",
    "Let us go ahead and run the ELT Data Pipeline using Dataproce Workflow Template and also validate to see if the Pipeline is successfully run as per the requirements. We will clean up all the workflow runs before taking care of the run and validation of the ELT Data Pipeline.\n",
    "* Step 1: Pre-run Validation\n",
    "* Step 2: Run ELT Data Pipeline using Dataproc Workflow Template\n",
    "* Step 3: Post-run Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.dataproc) Command name argument expected.\n",
      "\n",
      "\u001b[m\u001b[1mAvailable groups for gcloud dataproc:\u001b[m\n",
      "\n",
      "      autoscaling-policies    Create and manage Dataproc autoscaling policies.\n",
      "      batches                 Submit Dataproc batch jobs.\n",
      "      clusters                Create and manage Dataproc clusters.\n",
      "      jobs                    Submit and manage Dataproc jobs.\n",
      "      operations              View and manage Dataproc operations.\n",
      "      workflow-templates      Create and manage Dataproc workflow templates.\n",
      "\n",
      "\u001b[mFor detailed information on this command and its flags, run:\n",
      "  gcloud dataproc --help\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mERROR:\u001b[0m (gcloud.dataproc.operations) Command name argument expected.\n",
      "\n",
      "\u001b[m\u001b[1mAvailable commands for gcloud dataproc operations:\u001b[m\n",
      "\n",
      "      cancel                  Cancel an active operation.\n",
      "      delete                  Delete the record of an inactive operation.\n",
      "      describe                View the details of an operation.\n",
      "      get-iam-policy          Get IAM policy for an operation.\n",
      "      list                    View the list of all operations.\n",
      "      set-iam-policy          Set IAM policy for an operation.\n",
      "\n",
      "\u001b[mFor detailed information on this command and its flags, run:\n",
      "  gcloud dataproc operations --help\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                  TIMESTAMP                    TYPE      STATE  ERROR  WARNINGS\n",
      "b1724f85-0513-42f7-b98a-8afff9262f98  2023-06-12T23:03:42.065448Z  START     DONE\n",
      "83bd5bd6-9b96-40b0-b524-51200b7c6d50  2023-06-12T01:08:12.761540Z  STOP      DONE\n",
      "570eca31-a7b2-4614-969e-98dfa71edb41  2023-06-12T00:28:13.973864Z  WORKFLOW  DONE\n",
      "a280a791-f0ad-44c8-b3da-4a00b82a2686  2023-06-11T23:07:49.191932Z  WORKFLOW  DONE\n",
      "9f842dee-1f9f-4ba0-bbf7-ddaee65f132d  2023-06-11T22:52:19.754183Z  START     DONE\n",
      "bc6f0d6b-9d66-37bc-a6f5-0903d0d3fd2b  2023-06-11T01:12:50.149509Z  STOP      DONE\n",
      "79e2f790-e5ab-4a67-8c4d-c24dc7c40bbf  2023-06-10T23:28:07.504471Z  START     DONE\n",
      "27b858f2-49d1-4eb5-8727-f5ab0e2d72bb  2023-06-05T01:21:00.601008Z  STOP      DONE\n",
      "393741d3-ac47-4135-9eed-37bcbe0b853e  2023-06-04T22:37:12.885805Z  START     DONE\n",
      "e71b50aa-32a9-4739-9b6d-552653917f16  2023-06-04T16:25:22.378387Z  STOP      DONE\n",
      "4f8576e4-6113-44be-a662-0caec1920510  2023-06-04T15:07:47.701896Z  START     DONE\n",
      "6a0e3881-9840-450d-8f10-ded1ac0cc9ac  2023-05-29T01:02:29.024510Z  STOP      DONE\n",
      "7b74f213-e5d0-4a31-99b1-2cd698591013  2023-05-28T23:53:43.852042Z  START     DONE\n",
      "9b2f0541-1cf4-4001-a9fe-01da4ad6df0c  2023-05-28T21:45:04.216243Z  STOP      DONE\n",
      "b6f92b2f-86f5-4e68-99cf-85efec4330a4  2023-05-28T20:42:25.936244Z  START     DONE\n",
      "6f474c3d-4f07-4f46-ac80-62cc11882a1e  2023-05-28T17:18:17.982039Z  STOP      DONE\n",
      "0536c61e-6b3c-4894-8a34-90658a3730e3  2023-05-26T17:10:37.239798Z  CREATE    DONE          4\n",
      "786d3517-3cd6-4033-bdfc-530e741d6281  2023-05-20T23:52:35.641775Z  DELETE    DONE\n",
      "5f6abd23-d1db-4f6a-b342-e7480defbc39  2023-05-20T23:51:36.357712Z  STOP      DONE\n",
      "747f24b2-8d38-47d3-8d6a-1cb6ba165aa2  2023-05-20T21:48:49.171956Z  CREATE    DONE          4\n",
      "57816b80-ec7c-4085-a807-feccf302b3d3  2023-05-08T22:45:56.837554Z  DELETE    DONE\n",
      "fac15046-5023-45ab-83bd-659d5dac9efc  2023-05-07T23:14:32.416872Z  STOP      DONE\n",
      "d4940698-d8e1-49be-971d-901eec72a6fa  2023-05-07T22:57:35.605416Z  START     DONE\n",
      "4a473b6a-d737-4671-ab62-2659b22b90e4  2023-05-07T22:51:45.788082Z  STOP      DONE\n",
      "0d88047f-295b-456a-be2c-c49991c83bdf  2023-05-07T22:41:59.540485Z  CREATE    DONE          3\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc operations list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud dataproc operations describe d8fdbb7e-dcc5-32c2-bf70-ecd9f3c45ce6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud dataproc operations delete 812c4b24-b5e1-4657-a4a8-2783d5c4ed25 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [b738d3fcfc3748a98c873cafa26188c9] submitted.\n",
      "Waiting for job output...\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "23/06/12 23:15:19 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "23/06/12 23:15:19 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "23/06/12 23:15:19 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/06/12 23:15:19 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "Spark master: yarn, Application Id: application_1686611111024_0001\n",
      "Time taken: 6.489 seconds\n",
      "Time taken: 0.74 seconds\n",
      "23/06/12 23:15:36 WARN io.netty.channel.nio.NioEventLoop: Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@149a9b67.\n",
      "Job [b738d3fcfc3748a98c873cafa26188c9] finished successfully.\n",
      "done: true\n",
      "driverControlFilesUri: gs://dataproc-staging-us-central1-936546808722-ripwij5i/google-cloud-dataproc-metainfo/b012612f-63aa-49d4-b6f7-6397e09c7228/jobs/b738d3fcfc3748a98c873cafa26188c9/\n",
      "driverOutputResourceUri: gs://dataproc-staging-us-central1-936546808722-ripwij5i/google-cloud-dataproc-metainfo/b012612f-63aa-49d4-b6f7-6397e09c7228/jobs/b738d3fcfc3748a98c873cafa26188c9/driveroutput\n",
      "jobUuid: 185bfc50-5733-3936-b723-24203cfb8abd\n",
      "placement:\n",
      "  clusterName: cluster-256f\n",
      "  clusterUuid: b012612f-63aa-49d4-b6f7-6397e09c7228\n",
      "reference:\n",
      "  jobId: b738d3fcfc3748a98c873cafa26188c9\n",
      "  projectId: dataanalytics-347914\n",
      "sparkSqlJob:\n",
      "  queryFileUri: gs://airetail_mld/scripts/daily_product_revenue/cleanup.sql\n",
      "status:\n",
      "  state: DONE\n",
      "  stateStartTime: '2023-06-12T23:15:37.523769Z'\n",
      "statusHistory:\n",
      "- state: PENDING\n",
      "  stateStartTime: '2023-06-12T23:15:13.744469Z'\n",
      "- state: SETUP_DONE\n",
      "  stateStartTime: '2023-06-12T23:15:13.779170Z'\n",
      "- details: Agent reported job success\n",
      "  state: RUNNING\n",
      "  stateStartTime: '2023-06-12T23:15:14.099779Z'\n",
      "yarnApplications:\n",
      "- name: SparkSQL::10.128.0.4\n",
      "  progress: 1.0\n",
      "  state: FINISHED\n",
      "  trackingUrl: http://cluster-256f-m:8088/proxy/application_1686611111024_0001/\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc jobs submit spark-sql --cluster='cluster-256f' -f gs://airetail_mld/scripts/daily_product_revenue/cleanup.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2023-06-12T00:08:35.526710Z'\n",
      "id: wf-daily-product-revenue\n",
      "jobs:\n",
      "- sparkSqlJob:\n",
      "    queryFileUri: gs://airetail_mld/scripts/daily_product_revenue/cleanup.sql\n",
      "  stepId: job-cleanup\n",
      "- prerequisiteStepIds:\n",
      "  - job-cleanup\n",
      "  sparkSqlJob:\n",
      "    queryFileUri: gs://airetail_mld/scripts/daily_product_revenue/file_format_converter.sql\n",
      "    scriptVariables:\n",
      "      bucket_name: gs://airetail_mld\n",
      "      table_name: order_items\n",
      "  stepId: job-convert-order-items\n",
      "- prerequisiteStepIds:\n",
      "  - job-cleanup\n",
      "  sparkSqlJob:\n",
      "    queryFileUri: gs://airetail_mld/scripts/daily_product_revenue/file_format_converter.sql\n",
      "    scriptVariables:\n",
      "      bucket_name: gs://airetail_mld\n",
      "      table_name: orders\n",
      "  stepId: job-convert-orders\n",
      "- prerequisiteStepIds:\n",
      "  - job-convert-orders\n",
      "  - job-convert-order-items\n",
      "  sparkSqlJob:\n",
      "    queryFileUri: gs://airetail_mld/scripts/daily_product_revenue/compute_daily_product_revenue.sql\n",
      "    scriptVariables:\n",
      "      bucket_name: gs://airetail_mld\n",
      "  stepId: job-daily-product-revenue\n",
      "name: projects/dataanalytics-347914/regions/us-central1/workflowTemplates/wf-daily-product-revenue\n",
      "placement:\n",
      "  clusterSelector:\n",
      "    clusterLabels:\n",
      "      goog-dataproc-cluster-name: cluster-256f\n",
      "updateTime: '2023-06-12T00:18:39.027622Z'\n",
      "version: 6\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc workflow-templates describe wf-daily-product-revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting on operation [projects/dataanalytics-347914/regions/us-central1/operations/cb1801b2-eac8-30f4-abdb-7c9d6817455b].\n",
      "WorkflowTemplate [wf-daily-product-revenue] RUNNING\n",
      "Job ID job-cleanup-pieh4n6ulpbmg RUNNING\n",
      "Job ID job-cleanup-pieh4n6ulpbmg COMPLETED\n",
      "Job ID job-convert-order-items-pieh4n6ulpbmg RUNNING\n",
      "Job ID job-convert-orders-pieh4n6ulpbmg RUNNING\n",
      "Job ID job-convert-order-items-pieh4n6ulpbmg COMPLETED\n",
      "Job ID job-convert-orders-pieh4n6ulpbmg COMPLETED\n",
      "Job ID job-daily-product-revenue-pieh4n6ulpbmg RUNNING\n",
      "WorkflowTemplate [wf-daily-product-revenue] DONE\n",
      "Job ID job-daily-product-revenue-pieh4n6ulpbmg COMPLETED\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc workflow-templates instantiate wf-daily-product-revenue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [a8ed2a0584094ce2a9b9982de6a8ac21] submitted.\n",
      "Waiting for job output...\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "23/06/12 23:24:36 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "23/06/12 23:24:36 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "23/06/12 23:24:36 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/06/12 23:24:36 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "Spark master: yarn, Application Id: application_1686611111024_0006\n",
      "default\n",
      "retail_bronze_db\n",
      "retail_gold_db\n",
      "Time taken: 3.275 seconds, Fetched 3 row(s)\n",
      "Job [a8ed2a0584094ce2a9b9982de6a8ac21] finished successfully.\n",
      "done: true\n",
      "driverControlFilesUri: gs://dataproc-staging-us-central1-936546808722-ripwij5i/google-cloud-dataproc-metainfo/b012612f-63aa-49d4-b6f7-6397e09c7228/jobs/a8ed2a0584094ce2a9b9982de6a8ac21/\n",
      "driverOutputResourceUri: gs://dataproc-staging-us-central1-936546808722-ripwij5i/google-cloud-dataproc-metainfo/b012612f-63aa-49d4-b6f7-6397e09c7228/jobs/a8ed2a0584094ce2a9b9982de6a8ac21/driveroutput\n",
      "jobUuid: 9ec76e27-7eba-3d92-ad98-191ced2cc9f8\n",
      "placement:\n",
      "  clusterName: cluster-256f\n",
      "  clusterUuid: b012612f-63aa-49d4-b6f7-6397e09c7228\n",
      "reference:\n",
      "  jobId: a8ed2a0584094ce2a9b9982de6a8ac21\n",
      "  projectId: dataanalytics-347914\n",
      "sparkSqlJob:\n",
      "  queryList:\n",
      "    queries:\n",
      "    - SHOW databases\n",
      "status:\n",
      "  state: DONE\n",
      "  stateStartTime: '2023-06-12T23:24:52.105519Z'\n",
      "statusHistory:\n",
      "- state: PENDING\n",
      "  stateStartTime: '2023-06-12T23:24:31.710840Z'\n",
      "- state: SETUP_DONE\n",
      "  stateStartTime: '2023-06-12T23:24:31.741363Z'\n",
      "- details: Agent reported job success\n",
      "  state: RUNNING\n",
      "  stateStartTime: '2023-06-12T23:24:31.949394Z'\n",
      "yarnApplications:\n",
      "- name: SparkSQL::10.128.0.4\n",
      "  progress: 1.0\n",
      "  state: FINISHED\n",
      "  trackingUrl: http://cluster-256f-m:8088/proxy/application_1686611111024_0006/\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc jobs submit spark-sql --cluster='cluster-256f' -e \"SHOW databases;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [a7c2f9d297b94233af039915e0fefb3a] submitted.\n",
      "Waiting for job output...\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "23/06/12 23:25:22 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "23/06/12 23:25:22 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "23/06/12 23:25:22 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/06/12 23:25:22 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "Spark master: yarn, Application Id: application_1686611111024_0007\n",
      "Time taken: 2.198 seconds\n",
      "retail_bronze_db\torder_items\tfalse\n",
      "retail_bronze_db\torders\tfalse\n",
      "Time taken: 0.399 seconds, Fetched 2 row(s)\n",
      "Job [a7c2f9d297b94233af039915e0fefb3a] finished successfully.\n",
      "done: true\n",
      "driverControlFilesUri: gs://dataproc-staging-us-central1-936546808722-ripwij5i/google-cloud-dataproc-metainfo/b012612f-63aa-49d4-b6f7-6397e09c7228/jobs/a7c2f9d297b94233af039915e0fefb3a/\n",
      "driverOutputResourceUri: gs://dataproc-staging-us-central1-936546808722-ripwij5i/google-cloud-dataproc-metainfo/b012612f-63aa-49d4-b6f7-6397e09c7228/jobs/a7c2f9d297b94233af039915e0fefb3a/driveroutput\n",
      "jobUuid: 1eb959c6-5234-3ee3-ad28-a0164020c99a\n",
      "placement:\n",
      "  clusterName: cluster-256f\n",
      "  clusterUuid: b012612f-63aa-49d4-b6f7-6397e09c7228\n",
      "reference:\n",
      "  jobId: a7c2f9d297b94233af039915e0fefb3a\n",
      "  projectId: dataanalytics-347914\n",
      "sparkSqlJob:\n",
      "  queryList:\n",
      "    queries:\n",
      "    - USE retail_bronze_db;SHOW tables\n",
      "status:\n",
      "  state: DONE\n",
      "  stateStartTime: '2023-06-12T23:25:37.144450Z'\n",
      "statusHistory:\n",
      "- state: PENDING\n",
      "  stateStartTime: '2023-06-12T23:25:18.217937Z'\n",
      "- state: SETUP_DONE\n",
      "  stateStartTime: '2023-06-12T23:25:18.251322Z'\n",
      "- details: Agent reported job success\n",
      "  state: RUNNING\n",
      "  stateStartTime: '2023-06-12T23:25:18.448002Z'\n",
      "yarnApplications:\n",
      "- name: SparkSQL::10.128.0.4\n",
      "  progress: 1.0\n",
      "  state: FINISHED\n",
      "  trackingUrl: http://cluster-256f-m:8088/proxy/application_1686611111024_0007/\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc jobs submit spark-sql --cluster='cluster-256f' -e \"USE retail_bronze_db; SHOW tables;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [baddd9f56cfc4f28878bdeace3298440] submitted.\n",
      "Waiting for job output...\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "23/06/12 23:25:55 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "23/06/12 23:25:55 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "23/06/12 23:25:55 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/06/12 23:25:55 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "Spark master: yarn, Application Id: application_1686611111024_0008\n",
      "Time taken: 3.119 seconds\n",
      "68883\n",
      "Time taken: 7.2 seconds, Fetched 1 row(s)\n",
      "Job [baddd9f56cfc4f28878bdeace3298440] finished successfully.\n",
      "done: true\n",
      "driverControlFilesUri: gs://dataproc-staging-us-central1-936546808722-ripwij5i/google-cloud-dataproc-metainfo/b012612f-63aa-49d4-b6f7-6397e09c7228/jobs/baddd9f56cfc4f28878bdeace3298440/\n",
      "driverOutputResourceUri: gs://dataproc-staging-us-central1-936546808722-ripwij5i/google-cloud-dataproc-metainfo/b012612f-63aa-49d4-b6f7-6397e09c7228/jobs/baddd9f56cfc4f28878bdeace3298440/driveroutput\n",
      "jobUuid: f955ddbf-5a6d-3827-8f3c-eb67b7965de4\n",
      "placement:\n",
      "  clusterName: cluster-256f\n",
      "  clusterUuid: b012612f-63aa-49d4-b6f7-6397e09c7228\n",
      "reference:\n",
      "  jobId: baddd9f56cfc4f28878bdeace3298440\n",
      "  projectId: dataanalytics-347914\n",
      "sparkSqlJob:\n",
      "  queryList:\n",
      "    queries:\n",
      "    - USE retail_bronze_db;SELECT COUNT(*) FROM orders\n",
      "status:\n",
      "  state: DONE\n",
      "  stateStartTime: '2023-06-12T23:26:17.186146Z'\n",
      "statusHistory:\n",
      "- state: PENDING\n",
      "  stateStartTime: '2023-06-12T23:25:50.884207Z'\n",
      "- state: SETUP_DONE\n",
      "  stateStartTime: '2023-06-12T23:25:50.912175Z'\n",
      "- details: Agent reported job success\n",
      "  state: RUNNING\n",
      "  stateStartTime: '2023-06-12T23:25:51.122927Z'\n",
      "yarnApplications:\n",
      "- name: SparkSQL::10.128.0.4\n",
      "  progress: 1.0\n",
      "  state: FINISHED\n",
      "  trackingUrl: http://cluster-256f-m:8088/proxy/application_1686611111024_0008/\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc jobs submit spark-sql --cluster='cluster-256f' -e \"USE retail_bronze_db; SELECT COUNT(*) FROM orders;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [6367ec69a9a34684b368025570330d1f] submitted.\n",
      "Waiting for job output...\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "23/06/12 23:26:28 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "23/06/12 23:26:28 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "23/06/12 23:26:28 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/06/12 23:26:28 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "Spark master: yarn, Application Id: application_1686611111024_0009\n",
      "Time taken: 2.254 seconds\n",
      "retail_gold_db\tdaily_product_revenue\tfalse\n",
      "Time taken: 0.511 seconds, Fetched 1 row(s)\n",
      "Job [6367ec69a9a34684b368025570330d1f] finished successfully.\n",
      "done: true\n",
      "driverControlFilesUri: gs://dataproc-staging-us-central1-936546808722-ripwij5i/google-cloud-dataproc-metainfo/b012612f-63aa-49d4-b6f7-6397e09c7228/jobs/6367ec69a9a34684b368025570330d1f/\n",
      "driverOutputResourceUri: gs://dataproc-staging-us-central1-936546808722-ripwij5i/google-cloud-dataproc-metainfo/b012612f-63aa-49d4-b6f7-6397e09c7228/jobs/6367ec69a9a34684b368025570330d1f/driveroutput\n",
      "jobUuid: b7e1b837-708c-3c3a-8e09-4674c62a6689\n",
      "placement:\n",
      "  clusterName: cluster-256f\n",
      "  clusterUuid: b012612f-63aa-49d4-b6f7-6397e09c7228\n",
      "reference:\n",
      "  jobId: 6367ec69a9a34684b368025570330d1f\n",
      "  projectId: dataanalytics-347914\n",
      "sparkSqlJob:\n",
      "  queryList:\n",
      "    queries:\n",
      "    - USE retail_gold_db;SHOW tables\n",
      "status:\n",
      "  state: DONE\n",
      "  stateStartTime: '2023-06-12T23:26:42.213990Z'\n",
      "statusHistory:\n",
      "- state: PENDING\n",
      "  stateStartTime: '2023-06-12T23:26:23.937682Z'\n",
      "- state: SETUP_DONE\n",
      "  stateStartTime: '2023-06-12T23:26:23.968627Z'\n",
      "- details: Agent reported job success\n",
      "  state: RUNNING\n",
      "  stateStartTime: '2023-06-12T23:26:24.163617Z'\n",
      "yarnApplications:\n",
      "- name: SparkSQL::10.128.0.4\n",
      "  progress: 1.0\n",
      "  state: FINISHED\n",
      "  trackingUrl: http://cluster-256f-m:8088/proxy/application_1686611111024_0009/\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc jobs submit spark-sql --cluster='cluster-256f' -e \"USE retail_gold_db; SHOW tables;\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [90563088405c4ceeb1b2733b1ef09842] submitted.\n",
      "Waiting for job output...\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n",
      "23/06/12 23:26:58 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n",
      "23/06/12 23:26:58 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n",
      "23/06/12 23:26:58 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/06/12 23:26:58 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n",
      "Spark master: yarn, Application Id: application_1686611111024_0010\n",
      "Time taken: 2.149 seconds\n",
      "2013-07-29 00:00:00.0\t957\tPENDING_PAYMENT\t21\t6299.58\n",
      "2013-08-02 00:00:00.0\t1073\tCOMPLETE\t13\t2599.87\n",
      "2013-08-02 00:00:00.0\t885\tCLOSED\t7\t174.93\n",
      "2013-08-02 00:00:00.0\t282\tSUSPECTED_FRAUD\t5\t159.95\n",
      "2013-08-04 00:00:00.0\t1004\tCLOSED\t3\t1199.94\n",
      "2013-08-06 00:00:00.0\t793\tPROCESSING\t5\t74.95\n",
      "2013-08-09 00:00:00.0\t282\tPENDING_PAYMENT\t4\t127.96\n",
      "2013-08-10 00:00:00.0\t1073\tPENDING\t7\t1399.93\n",
      "2013-08-10 00:00:00.0\t502\tCLOSED\t8\t400.0\n",
      "2013-08-16 00:00:00.0\t403\tPENDING\t8\t1039.92\n",
      "Time taken: 5.046 seconds, Fetched 10 row(s)\n",
      "Job [90563088405c4ceeb1b2733b1ef09842] finished successfully.\n",
      "done: true\n",
      "driverControlFilesUri: gs://dataproc-staging-us-central1-936546808722-ripwij5i/google-cloud-dataproc-metainfo/b012612f-63aa-49d4-b6f7-6397e09c7228/jobs/90563088405c4ceeb1b2733b1ef09842/\n",
      "driverOutputResourceUri: gs://dataproc-staging-us-central1-936546808722-ripwij5i/google-cloud-dataproc-metainfo/b012612f-63aa-49d4-b6f7-6397e09c7228/jobs/90563088405c4ceeb1b2733b1ef09842/driveroutput\n",
      "jobUuid: a922c5df-5274-35c5-9cab-c4ab2d87d2d3\n",
      "placement:\n",
      "  clusterName: cluster-256f\n",
      "  clusterUuid: b012612f-63aa-49d4-b6f7-6397e09c7228\n",
      "reference:\n",
      "  jobId: 90563088405c4ceeb1b2733b1ef09842\n",
      "  projectId: dataanalytics-347914\n",
      "sparkSqlJob:\n",
      "  queryList:\n",
      "    queries:\n",
      "    - USE retail_gold_db;SELECT * FROM daily_product_revenue LIMIT 10\n",
      "status:\n",
      "  state: DONE\n",
      "  stateStartTime: '2023-06-12T23:27:17.228804Z'\n",
      "statusHistory:\n",
      "- state: PENDING\n",
      "  stateStartTime: '2023-06-12T23:26:53.522295Z'\n",
      "- state: SETUP_DONE\n",
      "  stateStartTime: '2023-06-12T23:26:53.549773Z'\n",
      "- details: Agent reported job success\n",
      "  state: RUNNING\n",
      "  stateStartTime: '2023-06-12T23:26:53.737257Z'\n",
      "yarnApplications:\n",
      "- name: SparkSQL::10.128.0.4\n",
      "  progress: 1.0\n",
      "  state: FINISHED\n",
      "  trackingUrl: http://cluster-256f-m:8088/proxy/application_1686611111024_0010/\n"
     ]
    }
   ],
   "source": [
    "!gcloud dataproc jobs submit spark-sql --cluster='cluster-256f' -e \"USE retail_gold_db; SELECT * FROM daily_product_revenue LIMIT 10;\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('deg-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a9d607f6995d470a72ac62c14cbba774ae3a8ede2bb7bb3a284130b245adccf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
