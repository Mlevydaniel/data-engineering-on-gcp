{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'dataanalytics-347914'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a query.\n",
    "QUERY = (\n",
    "    f'''SELECT * \n",
    "    FROM `{project_id}.retail.orders` \n",
    "    LIMIT 10'''\n",
    "    )\n",
    "query_job = client.query(QUERY)  # API request\n",
    "rows = query_job.result()  # Waits for query to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "google.cloud.bigquery.table.RowIterator"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for row in rows:\n",
    "    print(row.order_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_id = f\"{project_id}.retail.orders\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LoadJobConfig in module google.cloud.bigquery.job.load:\n",
      "\n",
      "class LoadJobConfig(google.cloud.bigquery.job.base._JobConfig)\n",
      " |  LoadJobConfig(**kwargs) -> None\n",
      " |  \n",
      " |  Configuration options for load jobs.\n",
      " |  \n",
      " |  Set properties on the constructed configuration by using the property name\n",
      " |  as the name of a keyword argument. Values which are unset or :data:`None`\n",
      " |  use the BigQuery REST API default values. See the `BigQuery REST API\n",
      " |  reference documentation\n",
      " |  <https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad>`_\n",
      " |  for a list of default values.\n",
      " |  \n",
      " |  Required options differ based on the\n",
      " |  :attr:`~google.cloud.bigquery.job.LoadJobConfig.source_format` value.\n",
      " |  For example, the BigQuery API's default value for\n",
      " |  :attr:`~google.cloud.bigquery.job.LoadJobConfig.source_format` is ``\"CSV\"``.\n",
      " |  When loading a CSV file, either\n",
      " |  :attr:`~google.cloud.bigquery.job.LoadJobConfig.schema` must be set or\n",
      " |  :attr:`~google.cloud.bigquery.job.LoadJobConfig.autodetect` must be set to\n",
      " |  :data:`True`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LoadJobConfig\n",
      " |      google.cloud.bigquery.job.base._JobConfig\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, **kwargs) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  allow_jagged_rows\n",
      " |      Optional[bool]: Allow missing trailing optional columns (CSV only).\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.allow_jagged_rows\n",
      " |  \n",
      " |  allow_quoted_newlines\n",
      " |      Optional[bool]: Allow quoted data containing newline characters (CSV only).\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.allow_quoted_newlines\n",
      " |  \n",
      " |  autodetect\n",
      " |      Optional[bool]: Automatically infer the schema from a sample of the data.\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.autodetect\n",
      " |  \n",
      " |  clustering_fields\n",
      " |      Optional[List[str]]: Fields defining clustering for the table\n",
      " |      \n",
      " |      (Defaults to :data:`None`).\n",
      " |      \n",
      " |      Clustering fields are immutable after table creation.\n",
      " |      \n",
      " |      .. note::\n",
      " |      \n",
      " |         BigQuery supports clustering for both partitioned and\n",
      " |         non-partitioned tables.\n",
      " |  \n",
      " |  connection_properties\n",
      " |      Connection properties.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.connection_properties\n",
      " |      \n",
      " |      .. versionadded:: 3.7.0\n",
      " |  \n",
      " |  create_disposition\n",
      " |      Optional[google.cloud.bigquery.job.CreateDisposition]: Specifies behavior\n",
      " |      for creating tables.\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.create_disposition\n",
      " |  \n",
      " |  create_session\n",
      " |      [Preview] If :data:`True`, creates a new session, where\n",
      " |      :attr:`~google.cloud.bigquery.job.LoadJob.session_info` will contain a\n",
      " |      random server generated session id.\n",
      " |      \n",
      " |      If :data:`False`, runs load job with an existing ``session_id`` passed in\n",
      " |      :attr:`~google.cloud.bigquery.job.LoadJobConfig.connection_properties`,\n",
      " |      otherwise runs load job in non-session mode.\n",
      " |      \n",
      " |      See\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.create_session\n",
      " |      \n",
      " |      .. versionadded:: 3.7.0\n",
      " |  \n",
      " |  decimal_target_types\n",
      " |      Possible SQL data types to which the source decimal values are converted.\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.decimal_target_types\n",
      " |      \n",
      " |      .. versionadded:: 2.21.0\n",
      " |  \n",
      " |  destination_encryption_configuration\n",
      " |      Optional[google.cloud.bigquery.encryption_configuration.EncryptionConfiguration]: Custom\n",
      " |      encryption configuration for the destination table.\n",
      " |      \n",
      " |      Custom encryption configuration (e.g., Cloud KMS keys) or :data:`None`\n",
      " |      if using default encryption.\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.destination_encryption_configuration\n",
      " |  \n",
      " |  destination_table_description\n",
      " |      Optional[str]: Description of the destination table.\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#DestinationTableProperties.FIELDS.description\n",
      " |  \n",
      " |  destination_table_friendly_name\n",
      " |      Optional[str]: Name given to destination table.\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#DestinationTableProperties.FIELDS.friendly_name\n",
      " |  \n",
      " |  encoding\n",
      " |      Optional[google.cloud.bigquery.job.Encoding]: The character encoding of the\n",
      " |      data.\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.encoding\n",
      " |  \n",
      " |  field_delimiter\n",
      " |      Optional[str]: The separator for fields in a CSV file.\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.field_delimiter\n",
      " |  \n",
      " |  hive_partitioning\n",
      " |      Optional[:class:`~.external_config.HivePartitioningOptions`]: [Beta] When set,         it configures hive partitioning support.\n",
      " |      \n",
      " |      .. note::\n",
      " |          **Experimental**. This feature is experimental and might change or\n",
      " |          have limited support.\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.hive_partitioning_options\n",
      " |  \n",
      " |  ignore_unknown_values\n",
      " |      Optional[bool]: Ignore extra values not represented in the table schema.\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.ignore_unknown_values\n",
      " |  \n",
      " |  max_bad_records\n",
      " |      Optional[int]: Number of invalid rows to ignore.\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.max_bad_records\n",
      " |  \n",
      " |  null_marker\n",
      " |      Optional[str]: Represents a null value (CSV only).\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.null_marker\n",
      " |  \n",
      " |  parquet_options\n",
      " |      Optional[google.cloud.bigquery.format_options.ParquetOptions]: Additional\n",
      " |          properties to set if ``sourceFormat`` is set to PARQUET.\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.parquet_options\n",
      " |  \n",
      " |  preserve_ascii_control_characters\n",
      " |      Optional[bool]: Preserves the embedded ASCII control characters when sourceFormat is set to CSV.\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.preserve_ascii_control_characters\n",
      " |  \n",
      " |  projection_fields\n",
      " |      Optional[List[str]]: If\n",
      " |      :attr:`google.cloud.bigquery.job.LoadJobConfig.source_format` is set to\n",
      " |      \"DATASTORE_BACKUP\", indicates which entity properties to load into\n",
      " |      BigQuery from a Cloud Datastore backup.\n",
      " |      \n",
      " |      Property names are case sensitive and must be top-level properties. If\n",
      " |      no properties are specified, BigQuery loads all properties. If any\n",
      " |      named property isn't found in the Cloud Datastore backup, an invalid\n",
      " |      error is returned in the job result.\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.projection_fields\n",
      " |  \n",
      " |  quote_character\n",
      " |      Optional[str]: Character used to quote data sections (CSV only).\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.quote\n",
      " |  \n",
      " |  range_partitioning\n",
      " |      Optional[google.cloud.bigquery.table.RangePartitioning]:\n",
      " |      Configures range-based partitioning for destination table.\n",
      " |      \n",
      " |      .. note::\n",
      " |          **Beta**. The integer range partitioning feature is in a\n",
      " |          pre-release state and might change or have limited support.\n",
      " |      \n",
      " |      Only specify at most one of\n",
      " |      :attr:`~google.cloud.bigquery.job.LoadJobConfig.time_partitioning` or\n",
      " |      :attr:`~google.cloud.bigquery.job.LoadJobConfig.range_partitioning`.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError:\n",
      " |              If the value is not\n",
      " |              :class:`~google.cloud.bigquery.table.RangePartitioning` or\n",
      " |              :data:`None`.\n",
      " |  \n",
      " |  reference_file_schema_uri\n",
      " |      Optional[str]:\n",
      " |      When creating an external table, the user can provide a reference file with the\n",
      " |      table schema. This is enabled for the following formats:\n",
      " |      \n",
      " |      AVRO, PARQUET, ORC\n",
      " |  \n",
      " |  schema\n",
      " |      Optional[Sequence[Union[             :class:`~google.cloud.bigquery.schema.SchemaField`,             Mapping[str, Any]         ]]]: Schema of the destination table.\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.schema\n",
      " |  \n",
      " |  schema_update_options\n",
      " |      Optional[List[google.cloud.bigquery.job.SchemaUpdateOption]]: Specifies\n",
      " |      updates to the destination table schema to allow as a side effect of\n",
      " |      the load job.\n",
      " |  \n",
      " |  skip_leading_rows\n",
      " |      Optional[int]: Number of rows to skip when reading data (CSV only).\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.skip_leading_rows\n",
      " |  \n",
      " |  source_format\n",
      " |      Optional[google.cloud.bigquery.job.SourceFormat]: File format of the data.\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.source_format\n",
      " |  \n",
      " |  time_partitioning\n",
      " |      Optional[google.cloud.bigquery.table.TimePartitioning]: Specifies time-based\n",
      " |      partitioning for the destination table.\n",
      " |      \n",
      " |      Only specify at most one of\n",
      " |      :attr:`~google.cloud.bigquery.job.LoadJobConfig.time_partitioning` or\n",
      " |      :attr:`~google.cloud.bigquery.job.LoadJobConfig.range_partitioning`.\n",
      " |  \n",
      " |  use_avro_logical_types\n",
      " |      Optional[bool]: For loads of Avro data, governs whether Avro logical types are\n",
      " |      converted to their corresponding BigQuery types (e.g. TIMESTAMP) rather than\n",
      " |      raw types (e.g. INTEGER).\n",
      " |  \n",
      " |  write_disposition\n",
      " |      Optional[google.cloud.bigquery.job.WriteDisposition]: Action that occurs if\n",
      " |      the destination table already exists.\n",
      " |      \n",
      " |      See:\n",
      " |      https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#JobConfigurationLoad.FIELDS.write_disposition\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from google.cloud.bigquery.job.base._JobConfig:\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Override to be able to raise error if an unknown property is being set\n",
      " |  \n",
      " |  to_api_repr(self) -> dict\n",
      " |      Build an API representation of the job config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Dict: A dictionary in the format used by the BigQuery API.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from google.cloud.bigquery.job.base._JobConfig:\n",
      " |  \n",
      " |  from_api_repr(resource: dict) -> '_JobConfig' from builtins.type\n",
      " |      Factory: construct a job configuration given its API representation\n",
      " |      \n",
      " |      Args:\n",
      " |          resource (Dict):\n",
      " |              A job configuration in the same representation as is returned\n",
      " |              from the API.\n",
      " |      \n",
      " |      Returns:\n",
      " |          google.cloud.bigquery.job._JobConfig: Configuration parsed from ``resource``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from google.cloud.bigquery.job.base._JobConfig:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  labels\n",
      " |      Dict[str, str]: Labels for the job.\n",
      " |      \n",
      " |      This method always returns a dict. Once a job has been created on the\n",
      " |      server, its labels cannot be modified anymore.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If ``value`` type is invalid.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(bigquery.LoadJobConfig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoadJobConfig helps to define the columns' data type that is going to be uploaded to the BigQuery table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=[\n",
    "        bigquery.SchemaField(\"order_id\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"order_date\", \"TIMESTAMP\"),\n",
    "        bigquery.SchemaField(\"order_customer_id\", \"INTEGER\"),\n",
    "        bigquery.SchemaField(\"order_status\", \"STRING\")\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://airetail_mld/retail_db/orders/part-00000\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://airetail_mld/retail_db/orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method load_table_from_uri in module google.cloud.bigquery.client:\n",
      "\n",
      "load_table_from_uri(source_uris: Union[str, Sequence[str]], destination: Union[google.cloud.bigquery.table.Table, google.cloud.bigquery.table.TableReference, google.cloud.bigquery.table.TableListItem, str], job_id: str = None, job_id_prefix: str = None, location: str = None, project: str = None, job_config: google.cloud.bigquery.job.load.LoadJobConfig = None, retry: google.api_core.retry.Retry = <google.api_core.retry.Retry object at 0x7fb048c771f0>, timeout: Optional[float] = None) -> google.cloud.bigquery.job.load.LoadJob method of google.cloud.bigquery.client.Client instance\n",
      "    Starts a job for loading data into a table from Cloud Storage.\n",
      "    \n",
      "    See\n",
      "    https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#jobconfigurationload\n",
      "    \n",
      "    Args:\n",
      "        source_uris (Union[str, Sequence[str]]):\n",
      "            URIs of data files to be loaded; in format\n",
      "            ``gs://<bucket_name>/<object_name_or_glob>``.\n",
      "        destination (Union[                 google.cloud.bigquery.table.Table,                 google.cloud.bigquery.table.TableReference,                 google.cloud.bigquery.table.TableListItem,                 str,             ]):\n",
      "            Table into which data is to be loaded. If a string is passed\n",
      "            in, this method attempts to create a table reference from a\n",
      "            string using\n",
      "            :func:`google.cloud.bigquery.table.TableReference.from_string`.\n",
      "    \n",
      "    Keyword Arguments:\n",
      "        job_id (Optional[str]): Name of the job.\n",
      "        job_id_prefix (Optional[str]):\n",
      "            The user-provided prefix for a randomly generated job ID.\n",
      "            This parameter will be ignored if a ``job_id`` is also given.\n",
      "        location (Optional[str]):\n",
      "            Location where to run the job. Must match the location of the\n",
      "            destination table.\n",
      "        project (Optional[str]):\n",
      "            Project ID of the project of where to run the job. Defaults\n",
      "            to the client's project.\n",
      "        job_config (Optional[google.cloud.bigquery.job.LoadJobConfig]):\n",
      "            Extra configuration options for the job.\n",
      "        retry (Optional[google.api_core.retry.Retry]):\n",
      "            How to retry the RPC.\n",
      "        timeout (Optional[float]):\n",
      "            The number of seconds to wait for the underlying HTTP transport\n",
      "            before using ``retry``.\n",
      "    \n",
      "    Returns:\n",
      "        google.cloud.bigquery.job.LoadJob: A new load job.\n",
      "    \n",
      "    Raises:\n",
      "        TypeError:\n",
      "            If ``job_config`` is not an instance of\n",
      "            :class:`~google.cloud.bigquery.job.LoadJobConfig` class.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(client.load_table_from_uri)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_table_from_uri helps loading data from GCS to BigQuery. It required 3 inputs\n",
    "\n",
    "1) uri\n",
    "2) table_id: table name\n",
    "3) job_config: here it goes the LoadJobConfig instance with the schema definition of the data to upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=dataanalytics-347914, location=US, id=d0b763a7-cc78-4f07-ae6e-97d2b800243e>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uri = \"gs://airetail_mld/retail_db/orders/part-00000\"\n",
    "\n",
    "# create job\n",
    "load_job = client.load_table_from_uri(\n",
    "    uri, table_id, job_config=job_config\n",
    ")  # Make an API request.\n",
    "\n",
    "# execute job\n",
    "load_job.result()  # Wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 68883 rows to table dataanalytics-347914.retail.orders\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# gets reference to table object, but it only has its metadata\n",
    "table = client.get_table(table_id)\n",
    "print(\"Loaded {} rows to table {}\".format(table.num_rows, table_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42834\n",
      "42851\n",
      "42864\n",
      "42888\n",
      "42890\n",
      "42896\n",
      "42898\n",
      "42899\n",
      "42906\n",
      "42913\n"
     ]
    }
   ],
   "source": [
    "# Perform a query.\n",
    "QUERY = (\n",
    "    f'''SELECT * FROM `{project_id}.retail.orders` LIMIT 10'''\n",
    ")\n",
    "query_job = client.query(QUERY)  # API request\n",
    "rows = query_job.result()  # Waits for query to finish\n",
    "\n",
    "for row in rows:\n",
    "    print(row.order_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query() method returns an interator that returns table.Row objects which has many attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'google.cloud.bigquery.table.Row'>\n",
      "<class 'google.cloud.bigquery.table.Row'>\n",
      "<class 'google.cloud.bigquery.table.Row'>\n",
      "<class 'google.cloud.bigquery.table.Row'>\n",
      "<class 'google.cloud.bigquery.table.Row'>\n",
      "<class 'google.cloud.bigquery.table.Row'>\n",
      "<class 'google.cloud.bigquery.table.Row'>\n",
      "<class 'google.cloud.bigquery.table.Row'>\n",
      "<class 'google.cloud.bigquery.table.Row'>\n",
      "<class 'google.cloud.bigquery.table.Row'>\n"
     ]
    }
   ],
   "source": [
    "# Perform a query.\n",
    "QUERY = (\n",
    "    f'''SELECT * FROM `{project_id}.retail.orders` LIMIT 10'''\n",
    ")\n",
    "query_job = client.query(QUERY)  # API request\n",
    "rows = query_job.result()  # Waits for query to finish\n",
    "\n",
    "for row in rows:\n",
    "    print(type(row))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the row data, we can use the values() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a query.\n",
    "QUERY = (\n",
    "    f'''SELECT * FROM `{project_id}.retail.orders` LIMIT 10'''\n",
    ")\n",
    "query_job = client.query(QUERY)  # API request\n",
    "rows = query_job.result()  # Waits for query to finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(42834,\n",
       "  datetime.datetime(2014, 4, 16, 0, 0, tzinfo=datetime.timezone.utc),\n",
       "  4321,\n",
       "  'CLOSED'),\n",
       " (42851,\n",
       "  datetime.datetime(2014, 4, 16, 0, 0, tzinfo=datetime.timezone.utc),\n",
       "  5972,\n",
       "  'CLOSED'),\n",
       " (42864,\n",
       "  datetime.datetime(2014, 4, 16, 0, 0, tzinfo=datetime.timezone.utc),\n",
       "  6024,\n",
       "  'CLOSED'),\n",
       " (42888,\n",
       "  datetime.datetime(2014, 4, 16, 0, 0, tzinfo=datetime.timezone.utc),\n",
       "  3712,\n",
       "  'CLOSED'),\n",
       " (42890,\n",
       "  datetime.datetime(2014, 4, 16, 0, 0, tzinfo=datetime.timezone.utc),\n",
       "  1961,\n",
       "  'CLOSED'),\n",
       " (42896,\n",
       "  datetime.datetime(2014, 4, 16, 0, 0, tzinfo=datetime.timezone.utc),\n",
       "  7705,\n",
       "  'CLOSED'),\n",
       " (42898,\n",
       "  datetime.datetime(2014, 4, 16, 0, 0, tzinfo=datetime.timezone.utc),\n",
       "  7770,\n",
       "  'CLOSED'),\n",
       " (42899,\n",
       "  datetime.datetime(2014, 4, 16, 0, 0, tzinfo=datetime.timezone.utc),\n",
       "  3119,\n",
       "  'CLOSED'),\n",
       " (42906,\n",
       "  datetime.datetime(2014, 4, 16, 0, 0, tzinfo=datetime.timezone.utc),\n",
       "  6215,\n",
       "  'CLOSED'),\n",
       " (42913,\n",
       "  datetime.datetime(2014, 4, 16, 0, 0, tzinfo=datetime.timezone.utc),\n",
       "  10692,\n",
       "  'CLOSED')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [r.values() for r in rows]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('COMPLETE', 22899)\n",
      "('PENDING_PAYMENT', 15030)\n",
      "('PROCESSING', 8275)\n",
      "('PENDING', 7610)\n",
      "('CLOSED', 7556)\n",
      "('ON_HOLD', 3798)\n",
      "('SUSPECTED_FRAUD', 1558)\n",
      "('CANCELED', 1428)\n",
      "('PAYMENT_REVIEW', 729)\n"
     ]
    }
   ],
   "source": [
    "# Perform a query.\n",
    "QUERY = (f'''\n",
    "    SELECT order_status, count(*) AS order_count\n",
    "    FROM `{project_id}.retail.orders`\n",
    "    GROUP BY 1\n",
    "    ORDER BY 2 DESC\n",
    "''')\n",
    "query_job = client.query(QUERY)  # API request\n",
    "rows = query_job.result()  # Waits for query to finish\n",
    "\n",
    "for row in rows:\n",
    "    print(row.values())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('deg-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4a9d607f6995d470a72ac62c14cbba774ae3a8ede2bb7bb3a284130b245adccf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
